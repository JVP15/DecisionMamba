{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "FJnwpdtXTuF7",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Training Decision Transformers with ðŸ¤— transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmRpOylHmoSo"
   },
   "source": [
    "In this tutorial, **youâ€™ll learn to train your first Offline Decision Transformer model from scratch to make a half-cheetah run.** ðŸƒ\n",
    "\n",
    "â“ If you have questions, please post them on #study-group discord channel ðŸ‘‰ https://discord.gg/aYka4Yhff9\n",
    "\n",
    "ðŸŽ® Environments:\n",
    "- [Half Cheetah](https://www.gymlibrary.dev/environments/mujoco/half_cheetah/)\n",
    "\n",
    "â¬‡ï¸ Here's what you'll achieve at the end of this tutorial. â¬‡ï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h78SBsjCviBm"
   },
   "source": [
    "### Prerequisites ðŸ—ï¸\n",
    "Before diving into the notebook, you need to:\n",
    "\n",
    "ðŸ”² ðŸ“š [Read the tutorial](https://huggingface.co/blog/train-decision-transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "DktITQNXTopc",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import DecisionTransformerConfig, DecisionTransformerModel, Trainer, TrainingArguments\n",
    "from modelling_mamba import MixerModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1Ugq2POUmRA"
   },
   "source": [
    "### Step 3: Loading the dataset from the ðŸ¤— Hub and instantiating the model\n",
    "\n",
    "We host a number of Offline RL Datasets on the hub. Today we will be training with the halfcheetah â€œexpertâ€ dataset, hosted here on hub.\n",
    "\n",
    "First we need to import the load_dataset function from the ðŸ¤— datasets package and download the dataset to our machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "s3bLeIHqUwq7",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\" # we diable weights and biases logging for this tutorial\n",
    "dataset = load_dataset(\"edbeeching/decision_transformer_gym_replay\", \"halfcheetah-expert-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFmTdHoHUD13"
   },
   "source": [
    "### Step 4: Defining a custom DataCollator for the transformers Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "l1QzZHmPUM4p",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DecisionTransformerGymDataCollator:\n",
    "    return_tensors: str = \"pt\"\n",
    "    max_len: int = 20 #subsets of the episode we use for training\n",
    "    state_dim: int = 17  # size of state space\n",
    "    act_dim: int = 6  # size of action space\n",
    "    max_ep_len: int = 1000 # max episode length in the dataset\n",
    "    scale: float = 1000.0  # normalization of rewards/returns\n",
    "    state_mean: np.array = None  # to store state means\n",
    "    state_std: np.array = None  # to store state stds\n",
    "    p_sample: np.array = None  # a distribution to take account trajectory lengths\n",
    "    n_traj: int = 0 # to store the number of trajectories in the dataset\n",
    "\n",
    "    def __init__(self, dataset) -> None:\n",
    "        self.act_dim = len(dataset[0][\"actions\"][0])\n",
    "        self.state_dim = len(dataset[0][\"observations\"][0])\n",
    "        self.dataset = dataset\n",
    "        # calculate dataset stats for normalization of states\n",
    "        states = []\n",
    "        traj_lens = []\n",
    "        for obs in dataset[\"observations\"]:\n",
    "            states.extend(obs)\n",
    "            traj_lens.append(len(obs))\n",
    "        self.n_traj = len(traj_lens)\n",
    "        states = np.vstack(states)\n",
    "        self.state_mean, self.state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "        \n",
    "        traj_lens = np.array(traj_lens)\n",
    "        self.p_sample = traj_lens / sum(traj_lens)\n",
    "\n",
    "    def _discount_cumsum(self, x, gamma):\n",
    "        discount_cumsum = np.zeros_like(x)\n",
    "        discount_cumsum[-1] = x[-1]\n",
    "        for t in reversed(range(x.shape[0] - 1)):\n",
    "            discount_cumsum[t] = x[t] + gamma * discount_cumsum[t + 1]\n",
    "        return discount_cumsum\n",
    "\n",
    "    def __call__(self, features):\n",
    "        batch_size = len(features)\n",
    "        # this is a bit of a hack to be able to sample of a non-uniform distribution\n",
    "        batch_inds = np.random.choice(\n",
    "            np.arange(self.n_traj),\n",
    "            size=batch_size,\n",
    "            replace=True,\n",
    "            p=self.p_sample,  # reweights so we sample according to timesteps\n",
    "        )\n",
    "        # a batch of dataset features\n",
    "        s, a, r, d, rtg, timesteps, mask = [], [], [], [], [], [], []\n",
    "        \n",
    "        for ind in batch_inds:\n",
    "            # for feature in features:\n",
    "            feature = self.dataset[int(ind)]\n",
    "            si = random.randint(0, len(feature[\"rewards\"]) - 1)\n",
    "\n",
    "            # get sequences from dataset\n",
    "            s.append(np.array(feature[\"observations\"][si : si + self.max_len]).reshape(1, -1, self.state_dim))\n",
    "            a.append(np.array(feature[\"actions\"][si : si + self.max_len]).reshape(1, -1, self.act_dim))\n",
    "            r.append(np.array(feature[\"rewards\"][si : si + self.max_len]).reshape(1, -1, 1))\n",
    "\n",
    "            d.append(np.array(feature[\"dones\"][si : si + self.max_len]).reshape(1, -1))\n",
    "            timesteps.append(np.arange(si, si + s[-1].shape[1]).reshape(1, -1))\n",
    "            timesteps[-1][timesteps[-1] >= self.max_ep_len] = self.max_ep_len - 1  # padding cutoff\n",
    "            rtg.append(\n",
    "                self._discount_cumsum(np.array(feature[\"rewards\"][si:]), gamma=1.0)[\n",
    "                    : s[-1].shape[1]   # TODO check the +1 removed here\n",
    "                ].reshape(1, -1, 1)\n",
    "            )\n",
    "            if rtg[-1].shape[1] < s[-1].shape[1]:\n",
    "                print(\"if true\")\n",
    "                rtg[-1] = np.concatenate([rtg[-1], np.zeros((1, 1, 1))], axis=1)\n",
    "\n",
    "            # padding and state + reward normalization\n",
    "            tlen = s[-1].shape[1]\n",
    "            s[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, self.state_dim)), s[-1]], axis=1)\n",
    "            s[-1] = (s[-1] - self.state_mean) / self.state_std\n",
    "            a[-1] = np.concatenate(\n",
    "                [np.ones((1, self.max_len - tlen, self.act_dim)) * -10.0, a[-1]],\n",
    "                axis=1,\n",
    "            )\n",
    "            r[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), r[-1]], axis=1)\n",
    "            d[-1] = np.concatenate([np.ones((1, self.max_len - tlen)) * 2, d[-1]], axis=1)\n",
    "            rtg[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), rtg[-1]], axis=1) / self.scale\n",
    "            timesteps[-1] = np.concatenate([np.zeros((1, self.max_len - tlen)), timesteps[-1]], axis=1)\n",
    "            mask.append(np.concatenate([np.zeros((1, self.max_len - tlen)), np.ones((1, tlen))], axis=1))\n",
    "\n",
    "        s = torch.from_numpy(np.concatenate(s, axis=0)).float()\n",
    "        a = torch.from_numpy(np.concatenate(a, axis=0)).float()\n",
    "        r = torch.from_numpy(np.concatenate(r, axis=0)).float()\n",
    "        d = torch.from_numpy(np.concatenate(d, axis=0))\n",
    "        rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).float()\n",
    "        timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).long()\n",
    "        mask = torch.from_numpy(np.concatenate(mask, axis=0)).float()\n",
    "\n",
    "        return {\n",
    "            \"states\": s,\n",
    "            \"actions\": a,\n",
    "            \"rewards\": r,\n",
    "            \"returns_to_go\": rtg,\n",
    "            \"timesteps\": timesteps,\n",
    "            \"attention_mask\": mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmTRGPKYUVFG"
   },
   "source": [
    "### Step 5: Extending the Decision Transformer Model to include a loss function\n",
    "\n",
    "In order to train the model with the ðŸ¤— trainer class, we first need to ensure the dictionary it returns contains a loss, in this case L-2 norm of the models action predictions and the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "bwZp7hhFUh5u",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "class TrainableDT(DecisionTransformerModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        output = super().forward(**kwargs)\n",
    "        # add the DT loss\n",
    "        action_preds = output[1]\n",
    "        action_targets = kwargs[\"actions\"]\n",
    "        attention_mask = kwargs[\"attention_mask\"]\n",
    "        act_dim = action_preds.shape[2]\n",
    "        action_preds = action_preds.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "        action_targets = action_targets.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "        \n",
    "        loss = torch.mean((action_preds - action_targets) ** 2)\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def original_forward(self, **kwargs):\n",
    "        return super().forward(**kwargs)\n",
    "\n",
    "class TrainableDM(TrainableDT):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.mamba = MixerModel(\n",
    "            d_model=config.hidden_size,\n",
    "            n_layer=24, # params from mamba-130m\n",
    "            ssm_cfg={},\n",
    "            rms_norm=True,\n",
    "            residual_in_fp32=True,\n",
    "            fused_add_norm=True,\n",
    "            #dtype=torch.bfloat16,\n",
    "            #device='cuda',\n",
    "        )\n",
    "\n",
    "        del self.encoder # get rid of the GPT-2 model\n",
    "        self.encoder = self._encoder_forward\n",
    "\n",
    "    def _encoder_forward(self, inputs_embeds, *args, **kwargs):\n",
    "        print(inputs_embeds.shape)\n",
    "        '''The original code expects a certain format for the inputs, but mamba only takes the input_embeds, so I made a wrapper around it that discards everything else'''\n",
    "        hidden_state = self.mamba(inputs_embeds)\n",
    "\n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "zIJCY3b3pQAh",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "collator = DecisionTransformerGymDataCollator(dataset[\"train\"])\n",
    "\n",
    "config = DecisionTransformerConfig(state_dim=collator.state_dim, act_dim=collator.act_dim)\n",
    "model = TrainableDM(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJJ2mr_cU4eE"
   },
   "source": [
    "### Step 6: Defining the training hyperparameters and training the model\n",
    "Here, we define the training hyperparameters and our Trainer class that we'll use to train our Decision Transformer model.\n",
    "\n",
    "This step takes about an hour, so you may leave it running. Note the authors train for at least 3 hours, so the results presented here are not as performant as the models hosted on the ðŸ¤— hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nNzzKWuuU9I4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 02:02, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=160, training_loss=0.19098453521728515, metrics={'train_runtime': 123.7889, 'train_samples_per_second': 80.783, 'train_steps_per_second': 1.293, 'total_flos': 12278352000000.0, 'train_loss': 0.19098453521728515, 'epoch': 10.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output/\",\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=10,\n",
    "    logging_strategy='epoch',\n",
    "    per_device_train_batch_size=64,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    optim=\"adamw_torch\",\n",
    "    max_grad_norm=0.25,\n",
    "    tf32=True,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNaj6bOkp3bt"
   },
   "source": [
    "### Step 7: Visualize the performance of the agent\n",
    "\n",
    "With mujoco_py, it'll take a little while to compile the first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "y0NhIn4up26c",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import mujoco\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "T-izl68BqUG5",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Function that gets an action from the model using autoregressive prediction with a window of the previous 20 timesteps.\n",
    "def get_action(model, states, actions, rewards, returns_to_go, timesteps):\n",
    "    # This implementation does not condition on past rewards\n",
    "\n",
    "    states = states.reshape(1, -1, model.config.state_dim)\n",
    "    actions = actions.reshape(1, -1, model.config.act_dim)\n",
    "    returns_to_go = returns_to_go.reshape(1, -1, 1)\n",
    "    timesteps = timesteps.reshape(1, -1)\n",
    "\n",
    "    states = states[:, -model.config.max_length :]\n",
    "    actions = actions[:, -model.config.max_length :]\n",
    "    returns_to_go = returns_to_go[:, -model.config.max_length :]\n",
    "    timesteps = timesteps[:, -model.config.max_length :]\n",
    "    padding = model.config.max_length - states.shape[1]\n",
    "    # pad all tokens to sequence length\n",
    "    attention_mask = torch.cat([torch.zeros(padding, device=device), torch.ones(states.shape[1], device=device)])\n",
    "    attention_mask = attention_mask.to(dtype=torch.long).reshape(1, -1)\n",
    "    states = torch.cat([torch.zeros((1, padding, model.config.state_dim), device=device), states], dim=1).float()\n",
    "    actions = torch.cat([torch.zeros((1, padding, model.config.act_dim), device=device), actions], dim=1).float()\n",
    "    returns_to_go = torch.cat([torch.zeros((1, padding, 1), device=device), returns_to_go], dim=1).float()\n",
    "    timesteps = torch.cat([torch.zeros((1, padding), dtype=torch.long, device=device), timesteps], dim=1)\n",
    "\n",
    "    state_preds, action_preds, return_preds = model.original_forward(\n",
    "        states=states,\n",
    "        actions=actions,\n",
    "        rewards=rewards,\n",
    "        returns_to_go=returns_to_go,\n",
    "        timesteps=timesteps,\n",
    "        attention_mask=attention_mask,\n",
    "        return_dict=False,\n",
    "    )\n",
    "\n",
    "    return action_preds[0, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "TFPuiNy-qWnP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04489212  0.03232612  0.06034821 -0.17081618 -0.19477023 -0.05751681\n",
      "  0.0970142   0.03239178 11.0473385  -0.07997213 -0.32363245  0.3629689\n",
      "  0.42323524  0.40836537  1.1085011  -0.48743752 -0.07375081]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jvp/miniconda3/envs/mamba/lib/python3.11/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001B[33mWARN: Overwriting existing videos at /mnt/c/Users/jorda/Documents/Python/DecisionMamba/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001B[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# build the environment\n",
    "directory = './video'\n",
    "device = \"cuda\"\n",
    "\n",
    "model = model.to(device)\n",
    "env = gym.make(\"HalfCheetah-v4\", render_mode='rgb_array')\n",
    "\n",
    "env = gym.wrappers.RecordVideo(env, directory)\n",
    "max_ep_len = 1000\n",
    "scale = 1000.0  # normalization for rewards/returns\n",
    "TARGET_RETURN = 12000 / scale  # evaluation is conditioned on a return of 12000, scaled accordingly\n",
    "\n",
    "state_mean = collator.state_mean.astype(np.float32)\n",
    "state_std = collator.state_std.astype(np.float32)\n",
    "print(state_mean)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "# Create the decision transformer model\n",
    "\n",
    "state_mean = torch.from_numpy(state_mean).to(device=device)\n",
    "state_std = torch.from_numpy(state_std).to(device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ysPH9rtRqY-g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n",
      "torch.Size([1, 60, 128])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 26\u001B[0m\n\u001B[1;32m     23\u001B[0m actions[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m action\n\u001B[1;32m     24\u001B[0m action \u001B[38;5;241m=\u001B[39m action\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mfloat32)\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m---> 26\u001B[0m state, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m cur_state \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(state)\u001B[38;5;241m.\u001B[39mto(device\u001B[38;5;241m=\u001B[39mdevice)\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m1\u001B[39m, state_dim)\n\u001B[1;32m     29\u001B[0m states \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([states, cur_state], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/mamba/lib/python3.11/site-packages/gymnasium/wrappers/record_video.py:183\u001B[0m, in \u001B[0;36mRecordVideo.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m    181\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrecording:\n\u001B[1;32m    182\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvideo_recorder \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 183\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvideo_recorder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcapture_frame\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrecorded_frames \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    185\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvideo_length \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/miniconda3/envs/mamba/lib/python3.11/site-packages/gymnasium/wrappers/monitoring/video_recorder.py:113\u001B[0m, in \u001B[0;36mVideoRecorder.capture_frame\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcapture_frame\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    112\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Render the given `env` and add the resulting frame to the video.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 113\u001B[0m     frame \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(frame, List):\n\u001B[1;32m    115\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_history \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m frame\n",
      "File \u001B[0;32m~/miniconda3/envs/mamba/lib/python3.11/site-packages/gymnasium/core.py:471\u001B[0m, in \u001B[0;36mWrapper.render\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    469\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrender\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m RenderFrame \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mlist\u001B[39m[RenderFrame] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    470\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 471\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/mamba/lib/python3.11/site-packages/gymnasium/wrappers/order_enforcing.py:70\u001B[0m, in \u001B[0;36mOrderEnforcing.render\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_disable_render_order_enforcing \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset:\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ResetNeeded(\n\u001B[1;32m     67\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     68\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     69\u001B[0m     )\n\u001B[0;32m---> 70\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/mamba/lib/python3.11/site-packages/gymnasium/wrappers/env_checker.py:67\u001B[0m, in \u001B[0;36mPassiveEnvChecker.render\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_render_passive_checker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 67\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/mamba/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_env.py:409\u001B[0m, in \u001B[0;36mMujocoEnv.render\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    408\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrender\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 409\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmujoco_renderer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    410\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender_mode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcamera_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcamera_name\u001B[49m\n\u001B[1;32m    411\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/mamba/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py:669\u001B[0m, in \u001B[0;36mMujocoRenderer.render\u001B[0;34m(self, render_mode, camera_id, camera_name)\u001B[0m\n\u001B[1;32m    662\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m camera_id \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    663\u001B[0m         camera_id \u001B[38;5;241m=\u001B[39m mujoco\u001B[38;5;241m.\u001B[39mmj_name2id(\n\u001B[1;32m    664\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel,\n\u001B[1;32m    665\u001B[0m             mujoco\u001B[38;5;241m.\u001B[39mmjtObj\u001B[38;5;241m.\u001B[39mmjOBJ_CAMERA,\n\u001B[1;32m    666\u001B[0m             camera_name,\n\u001B[1;32m    667\u001B[0m         )\n\u001B[0;32m--> 669\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[43mviewer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrender_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrender_mode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcamera_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcamera_id\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    670\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n\u001B[1;32m    672\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m render_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/miniconda3/envs/mamba/lib/python3.11/site-packages/gymnasium/envs/mujoco/mujoco_rendering.py:246\u001B[0m, in \u001B[0;36mOffScreenViewer.render\u001B[0;34m(self, render_mode, camera_id, segmentation)\u001B[0m\n\u001B[1;32m    239\u001B[0m rgb_arr \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros(\n\u001B[1;32m    240\u001B[0m     \u001B[38;5;241m3\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mviewport\u001B[38;5;241m.\u001B[39mwidth \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mviewport\u001B[38;5;241m.\u001B[39mheight, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39muint8\n\u001B[1;32m    241\u001B[0m )\n\u001B[1;32m    242\u001B[0m depth_arr \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros(\n\u001B[1;32m    243\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mviewport\u001B[38;5;241m.\u001B[39mwidth \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mviewport\u001B[38;5;241m.\u001B[39mheight, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32\n\u001B[1;32m    244\u001B[0m )\n\u001B[0;32m--> 246\u001B[0m \u001B[43mmujoco\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmjr_readPixels\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrgb_arr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdepth_arr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mviewport\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcon\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    248\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m render_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdepth_array\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    249\u001B[0m     depth_img \u001B[38;5;241m=\u001B[39m depth_arr\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mviewport\u001B[38;5;241m.\u001B[39mheight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mviewport\u001B[38;5;241m.\u001B[39mwidth)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Interact with the environment and create a video\n",
    "episode_return, episode_length = 0, 0\n",
    "state, info = env.reset()\n",
    "target_return = torch.tensor(TARGET_RETURN, device=device, dtype=torch.float32).reshape(1, 1)\n",
    "states = torch.from_numpy(state).reshape(1, state_dim).to(device=device, dtype=torch.float32)\n",
    "actions = torch.zeros((0, act_dim), device=device, dtype=torch.float32)\n",
    "rewards = torch.zeros(0, device=device, dtype=torch.float32)\n",
    "\n",
    "timesteps = torch.tensor(0, device=device, dtype=torch.long).reshape(1, 1)\n",
    "for t in range(max_ep_len):\n",
    "    actions = torch.cat([actions, torch.zeros((1, act_dim), device=device)], dim=0)\n",
    "    rewards = torch.cat([rewards, torch.zeros(1, device=device)])\n",
    "\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        action = get_action(\n",
    "            model,\n",
    "            (states - state_mean) / state_std,\n",
    "            actions,\n",
    "            rewards,\n",
    "            target_return,\n",
    "            timesteps,\n",
    "        )\n",
    "    actions[-1] = action\n",
    "    action = action.detach().to(torch.float32).cpu().numpy()\n",
    "\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    cur_state = torch.from_numpy(state).to(device=device).reshape(1, state_dim)\n",
    "    states = torch.cat([states, cur_state], dim=0)\n",
    "    rewards[-1] = reward\n",
    "\n",
    "    pred_return = target_return[0, -1] - (reward / scale)\n",
    "    target_return = torch.cat([target_return, pred_return.reshape(1, 1)], dim=1)\n",
    "    timesteps = torch.cat([timesteps, torch.ones((1, 1), device=device, dtype=torch.long) * (t + 1)], dim=1)\n",
    "\n",
    "    episode_return += reward\n",
    "    episode_length += 1\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
