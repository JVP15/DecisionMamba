{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "FJnwpdtXTuF7",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Training Decision Transformers with ðŸ¤— transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmRpOylHmoSo"
   },
   "source": [
    "In this tutorial, **youâ€™ll learn to train your first Offline Decision Transformer model from scratch to make a half-cheetah run.** ðŸƒ\n",
    "\n",
    "â“ If you have questions, please post them on #study-group discord channel ðŸ‘‰ https://discord.gg/aYka4Yhff9\n",
    "\n",
    "ðŸŽ® Environments:\n",
    "- [Half Cheetah](https://www.gymlibrary.dev/environments/mujoco/half_cheetah/)\n",
    "\n",
    "â¬‡ï¸ Here's what you'll achieve at the end of this tutorial. â¬‡ï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h78SBsjCviBm"
   },
   "source": [
    "### Prerequisites ðŸ—ï¸\n",
    "Before diving into the notebook, you need to:\n",
    "\n",
    "ðŸ”² ðŸ“š [Read the tutorial](https://huggingface.co/blog/train-decision-transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DktITQNXTopc",
    "ExecuteTime": {
     "end_time": "2023-12-19T02:43:28.357717016Z",
     "start_time": "2023-12-19T02:43:24.217733454Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import DecisionTransformerConfig, DecisionTransformerModel, Trainer, TrainingArguments\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "from modelling_mamba import MixerModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1Ugq2POUmRA"
   },
   "source": [
    "### Step 3: Loading the dataset from the ðŸ¤— Hub and instantiating the model\n",
    "\n",
    "We host a number of Offline RL Datasets on the hub. Today we will be training with the halfcheetah â€œexpertâ€ dataset, hosted here on hub.\n",
    "\n",
    "First we need to import the load_dataset function from the ðŸ¤— datasets package and download the dataset to our machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "s3bLeIHqUwq7",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\" # we diable weights and biases logging for this tutorial\n",
    "dataset = load_dataset(\"edbeeching/decision_transformer_gym_replay\", \"halfcheetah-expert-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFmTdHoHUD13"
   },
   "source": [
    "### Step 4: Defining a custom DataCollator for the transformers Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "l1QzZHmPUM4p",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DecisionTransformerGymDataCollator:\n",
    "    return_tensors: str = \"pt\"\n",
    "    max_len: int = 20 #subsets of the episode we use for training\n",
    "    state_dim: int = 17  # size of state space\n",
    "    act_dim: int = 6  # size of action space\n",
    "    max_ep_len: int = 1000 # max episode length in the dataset\n",
    "    scale: float = 1000.0  # normalization of rewards/returns\n",
    "    state_mean: np.array = None  # to store state means\n",
    "    state_std: np.array = None  # to store state stds\n",
    "    p_sample: np.array = None  # a distribution to take account trajectory lengths\n",
    "    n_traj: int = 0 # to store the number of trajectories in the dataset\n",
    "\n",
    "    def __init__(self, dataset) -> None:\n",
    "        self.act_dim = len(dataset[0][\"actions\"][0])\n",
    "        self.state_dim = len(dataset[0][\"observations\"][0])\n",
    "        self.dataset = dataset\n",
    "        # calculate dataset stats for normalization of states\n",
    "        states = []\n",
    "        traj_lens = []\n",
    "        for obs in dataset[\"observations\"]:\n",
    "            states.extend(obs)\n",
    "            traj_lens.append(len(obs))\n",
    "        self.n_traj = len(traj_lens)\n",
    "        states = np.vstack(states)\n",
    "        self.state_mean, self.state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "        \n",
    "        traj_lens = np.array(traj_lens)\n",
    "        self.p_sample = traj_lens / sum(traj_lens)\n",
    "\n",
    "    def _discount_cumsum(self, x, gamma):\n",
    "        discount_cumsum = np.zeros_like(x)\n",
    "        discount_cumsum[-1] = x[-1]\n",
    "        for t in reversed(range(x.shape[0] - 1)):\n",
    "            discount_cumsum[t] = x[t] + gamma * discount_cumsum[t + 1]\n",
    "        return discount_cumsum\n",
    "\n",
    "    def __call__(self, features):\n",
    "        batch_size = len(features)\n",
    "        # this is a bit of a hack to be able to sample of a non-uniform distribution\n",
    "        batch_inds = np.random.choice(\n",
    "            np.arange(self.n_traj),\n",
    "            size=batch_size,\n",
    "            replace=True,\n",
    "            p=self.p_sample,  # reweights so we sample according to timesteps\n",
    "        )\n",
    "        # a batch of dataset features\n",
    "        s, a, r, d, rtg, timesteps, mask = [], [], [], [], [], [], []\n",
    "        \n",
    "        for ind in batch_inds:\n",
    "            # for feature in features:\n",
    "            feature = self.dataset[int(ind)]\n",
    "            si = random.randint(0, len(feature[\"rewards\"]) - 1)\n",
    "\n",
    "            # get sequences from dataset\n",
    "            s.append(np.array(feature[\"observations\"][si : si + self.max_len]).reshape(1, -1, self.state_dim))\n",
    "            a.append(np.array(feature[\"actions\"][si : si + self.max_len]).reshape(1, -1, self.act_dim))\n",
    "            r.append(np.array(feature[\"rewards\"][si : si + self.max_len]).reshape(1, -1, 1))\n",
    "\n",
    "            d.append(np.array(feature[\"dones\"][si : si + self.max_len]).reshape(1, -1))\n",
    "            timesteps.append(np.arange(si, si + s[-1].shape[1]).reshape(1, -1))\n",
    "            timesteps[-1][timesteps[-1] >= self.max_ep_len] = self.max_ep_len - 1  # padding cutoff\n",
    "            rtg.append(\n",
    "                self._discount_cumsum(np.array(feature[\"rewards\"][si:]), gamma=1.0)[\n",
    "                    : s[-1].shape[1]   # TODO check the +1 removed here\n",
    "                ].reshape(1, -1, 1)\n",
    "            )\n",
    "            if rtg[-1].shape[1] < s[-1].shape[1]:\n",
    "                print(\"if true\")\n",
    "                rtg[-1] = np.concatenate([rtg[-1], np.zeros((1, 1, 1))], axis=1)\n",
    "\n",
    "            # padding and state + reward normalization\n",
    "            tlen = s[-1].shape[1]\n",
    "            s[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, self.state_dim)), s[-1]], axis=1)\n",
    "            s[-1] = (s[-1] - self.state_mean) / self.state_std\n",
    "            a[-1] = np.concatenate(\n",
    "                [np.ones((1, self.max_len - tlen, self.act_dim)) * -10.0, a[-1]],\n",
    "                axis=1,\n",
    "            )\n",
    "            r[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), r[-1]], axis=1)\n",
    "            d[-1] = np.concatenate([np.ones((1, self.max_len - tlen)) * 2, d[-1]], axis=1)\n",
    "            rtg[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), rtg[-1]], axis=1) / self.scale\n",
    "            timesteps[-1] = np.concatenate([np.zeros((1, self.max_len - tlen)), timesteps[-1]], axis=1)\n",
    "            mask.append(np.concatenate([np.zeros((1, self.max_len - tlen)), np.ones((1, tlen))], axis=1))\n",
    "\n",
    "        s = torch.from_numpy(np.concatenate(s, axis=0)).float()\n",
    "        a = torch.from_numpy(np.concatenate(a, axis=0)).float()\n",
    "        r = torch.from_numpy(np.concatenate(r, axis=0)).float()\n",
    "        d = torch.from_numpy(np.concatenate(d, axis=0))\n",
    "        rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).float()\n",
    "        timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).long()\n",
    "        mask = torch.from_numpy(np.concatenate(mask, axis=0)).float()\n",
    "\n",
    "        return {\n",
    "            \"states\": s,\n",
    "            \"actions\": a,\n",
    "            \"rewards\": r,\n",
    "            \"returns_to_go\": rtg,\n",
    "            \"timesteps\": timesteps,\n",
    "            \"attention_mask\": mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmTRGPKYUVFG"
   },
   "source": [
    "### Step 5: Extending the Decision Transformer Model to include a loss function\n",
    "\n",
    "In order to train the model with the ðŸ¤— trainer class, we first need to ensure the dictionary it returns contains a loss, in this case L-2 norm of the models action predictions and the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bwZp7hhFUh5u",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "class TrainableDT(DecisionTransformerModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        output = super().forward(**kwargs)\n",
    "        # add the DT loss\n",
    "        action_preds = output[1]\n",
    "        action_targets = kwargs[\"actions\"]\n",
    "        attention_mask = kwargs[\"attention_mask\"]\n",
    "        act_dim = action_preds.shape[2]\n",
    "        action_preds = action_preds.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "        action_targets = action_targets.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "        \n",
    "        loss = torch.mean((action_preds - action_targets) ** 2)\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def original_forward(self, **kwargs):\n",
    "        return super().forward(**kwargs)\n",
    "\n",
    "class TrainableDM(TrainableDT):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.mamba = MixerModel(\n",
    "            d_model=config.hidden_size,\n",
    "            n_layer=6, # params from mamba-130m\n",
    "            ssm_cfg={},\n",
    "            rms_norm=True,\n",
    "            residual_in_fp32=True,\n",
    "            fused_add_norm=True,\n",
    "            #dtype=torch.bfloat16,\n",
    "            #device='cuda',\n",
    "        )\n",
    "\n",
    "        del self.encoder # get rid of the GPT-2 model\n",
    "        self.encoder = self._encoder_forward\n",
    "\n",
    "    def _encoder_forward(self, inputs_embeds, *args, **kwargs):\n",
    "        hidden_states = self.mamba(inputs_embeds)\n",
    "\n",
    "        return BaseModelOutput(last_hidden_state=hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zIJCY3b3pQAh",
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1257368"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collator = DecisionTransformerGymDataCollator(dataset[\"train\"])\n",
    "\n",
    "config = DecisionTransformerConfig(state_dim=collator.state_dim, act_dim=collator.act_dim)\n",
    "model = TrainableDT(config)\n",
    "\n",
    "def num_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "num_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJJ2mr_cU4eE"
   },
   "source": [
    "### Step 6: Defining the training hyperparameters and training the model\n",
    "Here, we define the training hyperparameters and our Trainer class that we'll use to train our Decision Transformer model.\n",
    "\n",
    "This step takes about an hour, so you may leave it running. Note the authors train for at least 3 hours, so the results presented here are not as performant as the models hosted on the ðŸ¤— hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nNzzKWuuU9I4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100000' max='100000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100000/100000 4:26:21, Epoch 6250/6250]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.042500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.030300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.029500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.028900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.028500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.028200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.027900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.027700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.027500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.027300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.027200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>0.026800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.026700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>0.026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.026500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95000</td>\n",
       "      <td>0.026400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.026300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100000, training_loss=0.028536181030273437, metrics={'train_runtime': 15983.9549, 'train_samples_per_second': 400.402, 'train_steps_per_second': 6.256, 'total_flos': 7673970000000000.0, 'train_loss': 0.028536181030273437, 'epoch': 6250.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these params more or less match the ones used by the original DT paper\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output/\",\n",
    "    remove_unused_columns=False,\n",
    "    #num_train_epochs=120,\n",
    "    max_steps=100_000,\n",
    "    logging_strategy='steps',\n",
    "    save_strategy='no',\n",
    "    logging_steps=5000,\n",
    "    warmup_steps=0,\n",
    "    per_device_train_batch_size=64,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    #warmup_ratio=0.1,\n",
    "    optim=\"adamw_torch\",\n",
    "    dataloader_num_workers=12,\n",
    "    max_grad_norm=0.25,\n",
    "    tf32=True,\n",
    "    bf16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Mamba Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100000' max='100000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100000/100000 4:38:50, Epoch 6250/6250]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.027500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.026100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.025300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.024700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.024200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.023800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.023400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.022700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.022400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.022100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.021800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.021600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>0.021400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.021300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>0.021100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.021000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95000</td>\n",
       "      <td>0.020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.020800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100000, training_loss=0.023592724838256834, metrics={'train_runtime': 16736.1936, 'train_samples_per_second': 382.405, 'train_steps_per_second': 5.975, 'total_flos': 9008946000000000.0, 'train_loss': 0.023592724838256834, 'epoch': 6250.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = DecisionTransformerConfig(state_dim=collator.state_dim, act_dim=collator.act_dim)\n",
    "model = TrainableDM(config)\n",
    "\n",
    "def num_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "num_params(model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNaj6bOkp3bt"
   },
   "source": [
    "### Step 7: Visualize the performance of the agent\n",
    "\n",
    "With mujoco_py, it'll take a little while to compile the first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "y0NhIn4up26c",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import mujoco\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "T-izl68BqUG5",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Function that gets an action from the model using autoregressive prediction with a window of the previous 20 timesteps.\n",
    "def get_action(model, states, actions, rewards, returns_to_go, timesteps):\n",
    "    # This implementation does not condition on past rewards\n",
    "\n",
    "    states = states.reshape(1, -1, model.config.state_dim)\n",
    "    actions = actions.reshape(1, -1, model.config.act_dim)\n",
    "    returns_to_go = returns_to_go.reshape(1, -1, 1)\n",
    "    timesteps = timesteps.reshape(1, -1)\n",
    "\n",
    "    states = states[:, -model.config.max_length :]\n",
    "    actions = actions[:, -model.config.max_length :]\n",
    "    returns_to_go = returns_to_go[:, -model.config.max_length :]\n",
    "    timesteps = timesteps[:, -model.config.max_length :]\n",
    "    padding = model.config.max_length - states.shape[1]\n",
    "    # pad all tokens to sequence length\n",
    "    attention_mask = torch.cat([torch.zeros(padding, device=device), torch.ones(states.shape[1], device=device)])\n",
    "    attention_mask = attention_mask.to(dtype=torch.long).reshape(1, -1)\n",
    "    states = torch.cat([torch.zeros((1, padding, model.config.state_dim), device=device), states], dim=1).float()\n",
    "    actions = torch.cat([torch.zeros((1, padding, model.config.act_dim), device=device), actions], dim=1).float()\n",
    "    returns_to_go = torch.cat([torch.zeros((1, padding, 1), device=device), returns_to_go], dim=1).float()\n",
    "    timesteps = torch.cat([torch.zeros((1, padding), dtype=torch.long, device=device), timesteps], dim=1)\n",
    "\n",
    "    state_preds, action_preds, return_preds = model.original_forward(\n",
    "        states=states,\n",
    "        actions=actions,\n",
    "        rewards=rewards,\n",
    "        returns_to_go=returns_to_go,\n",
    "        timesteps=timesteps,\n",
    "        attention_mask=attention_mask,\n",
    "        return_dict=False,\n",
    "    )\n",
    "\n",
    "    return action_preds[0, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TFPuiNy-qWnP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04489212  0.03232612  0.06034821 -0.17081618 -0.19477023 -0.05751681\n",
      "  0.0970142   0.03239178 11.0473385  -0.07997213 -0.32363245  0.3629689\n",
      "  0.42323524  0.40836537  1.1085011  -0.48743752 -0.07375081]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jvp/miniconda3/envs/mamba/lib/python3.11/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001B[33mWARN: Overwriting existing videos at /mnt/c/Users/jorda/Documents/Python/DecisionMamba/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001B[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# build the environment\n",
    "directory = './video'\n",
    "device = \"cuda\"\n",
    "\n",
    "model = model.to(device)\n",
    "env = gym.make(\"HalfCheetah-v4\", render_mode='rgb_array')\n",
    "\n",
    "env = gym.wrappers.RecordVideo(env, directory)\n",
    "max_ep_len = 1000\n",
    "scale = 1000.0  # normalization for rewards/returns\n",
    "TARGET_RETURN = 12000 / scale  # evaluation is conditioned on a return of 12000, scaled accordingly\n",
    "\n",
    "state_mean = collator.state_mean.astype(np.float32)\n",
    "state_std = collator.state_std.astype(np.float32)\n",
    "print(state_mean)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "# Create the decision transformer model\n",
    "\n",
    "state_mean = torch.from_numpy(state_mean).to(device=device)\n",
    "state_std = torch.from_numpy(state_std).to(device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ysPH9rtRqY-g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /mnt/c/Users/jorda/Documents/Python/DecisionMamba/video/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /mnt/c/Users/jorda/Documents/Python/DecisionMamba/video/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /mnt/c/Users/jorda/Documents/Python/DecisionMamba/video/rl-video-episode-0.mp4\n"
     ]
    }
   ],
   "source": [
    "# Interact with the environment and create a video\n",
    "episode_return, episode_length = 0, 0\n",
    "state, info = env.reset()\n",
    "target_return = torch.tensor(TARGET_RETURN, device=device, dtype=torch.float32).reshape(1, 1)\n",
    "states = torch.from_numpy(state).reshape(1, state_dim).to(device=device, dtype=torch.float32)\n",
    "actions = torch.zeros((0, act_dim), device=device, dtype=torch.float32)\n",
    "rewards = torch.zeros(0, device=device, dtype=torch.float32)\n",
    "\n",
    "timesteps = torch.tensor(0, device=device, dtype=torch.long).reshape(1, 1)\n",
    "for t in range(max_ep_len):\n",
    "    actions = torch.cat([actions, torch.zeros((1, act_dim), device=device)], dim=0)\n",
    "    rewards = torch.cat([rewards, torch.zeros(1, device=device)])\n",
    "\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        action = get_action(\n",
    "            model,\n",
    "            (states - state_mean) / state_std,\n",
    "            actions,\n",
    "            rewards,\n",
    "            target_return,\n",
    "            timesteps,\n",
    "        )\n",
    "    actions[-1] = action\n",
    "    action = action.detach().to(torch.float32).cpu().numpy()\n",
    "\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    cur_state = torch.from_numpy(state).to(device=device).reshape(1, state_dim)\n",
    "    states = torch.cat([states, cur_state], dim=0)\n",
    "    rewards[-1] = reward\n",
    "\n",
    "    pred_return = target_return[0, -1] - (reward / scale)\n",
    "    target_return = torch.cat([target_return, pred_return.reshape(1, 1)], dim=1)\n",
    "    timesteps = torch.cat([timesteps, torch.ones((1, 1), device=device, dtype=torch.long) * (t + 1)], dim=1)\n",
    "\n",
    "    episode_return += reward\n",
    "    episode_length += 1\n",
    "\n",
    "    if terminated or truncated:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
