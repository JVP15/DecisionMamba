{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "FJnwpdtXTuF7",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Training Decision Transformers with ðŸ¤— transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmRpOylHmoSo"
   },
   "source": [
    "In this tutorial, **youâ€™ll learn to train your first Offline Decision Transformer model from scratch to make a half-cheetah run.** ðŸƒ\n",
    "\n",
    "â“ If you have questions, please post them on #study-group discord channel ðŸ‘‰ https://discord.gg/aYka4Yhff9\n",
    "\n",
    "ðŸŽ® Environments:\n",
    "- [Half Cheetah](https://www.gymlibrary.dev/environments/mujoco/half_cheetah/)\n",
    "\n",
    "â¬‡ï¸ Here's what you'll achieve at the end of this tutorial. â¬‡ï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h78SBsjCviBm"
   },
   "source": [
    "### Prerequisites ðŸ—ï¸\n",
    "Before diving into the notebook, you need to:\n",
    "\n",
    "ðŸ”² ðŸ“š [Read the tutorial](https://huggingface.co/blog/train-decision-transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T03:30:19.985380913Z",
     "start_time": "2023-12-20T03:30:16.578655959Z"
    },
    "id": "DktITQNXTopc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import DecisionTransformerConfig, Trainer, TrainingArguments, TrainerCallback\n",
    "from models.decision_mamba import TrainableDT, TrainableDM\n",
    "from evaluation.evaluate_episodes import evaluate_episode_rtg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1Ugq2POUmRA"
   },
   "source": [
    "### Step 3: Loading the dataset from the ðŸ¤— Hub and instantiating the model\n",
    "\n",
    "We host a number of Offline RL Datasets on the hub. Today we will be training with the halfcheetah â€œexpertâ€ dataset, hosted here on hub.\n",
    "\n",
    "First we need to import the load_dataset function from the ðŸ¤— datasets package and download the dataset to our machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "s3bLeIHqUwq7",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\" # we diable weights and biases logging for this tutorial\n",
    "dataset = load_dataset(\"edbeeching/decision_transformer_gym_replay\", \"halfcheetah-expert-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFmTdHoHUD13"
   },
   "source": [
    "### Step 4: Defining a custom DataCollator for the transformers Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "l1QzZHmPUM4p",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DecisionTransformerGymDataCollator:\n",
    "    return_tensors: str = \"pt\"\n",
    "    max_len: int = 20 #subsets of the episode we use for training\n",
    "    state_dim: int = 17  # size of state space\n",
    "    act_dim: int = 6  # size of action space\n",
    "    max_ep_len: int = 1000 # max episode length in the dataset\n",
    "    scale: float = 1000.0  # normalization of rewards/returns\n",
    "    state_mean: np.array = None  # to store state means\n",
    "    state_std: np.array = None  # to store state stds\n",
    "    p_sample: np.array = None  # a distribution to take account trajectory lengths\n",
    "    n_traj: int = 0 # to store the number of trajectories in the dataset\n",
    "\n",
    "    def __init__(self, dataset) -> None:\n",
    "        self.act_dim = len(dataset[0][\"actions\"][0])\n",
    "        self.state_dim = len(dataset[0][\"observations\"][0])\n",
    "        self.dataset = dataset\n",
    "        # calculate dataset stats for normalization of states\n",
    "        states = []\n",
    "        traj_lens = []\n",
    "        for obs in dataset[\"observations\"]:\n",
    "            states.extend(obs)\n",
    "            traj_lens.append(len(obs))\n",
    "        self.n_traj = len(traj_lens)\n",
    "        states = np.vstack(states)\n",
    "        self.state_mean, self.state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "        \n",
    "        traj_lens = np.array(traj_lens)\n",
    "        self.p_sample = traj_lens / sum(traj_lens)\n",
    "\n",
    "    def _discount_cumsum(self, x, gamma):\n",
    "        discount_cumsum = np.zeros_like(x)\n",
    "        discount_cumsum[-1] = x[-1]\n",
    "        for t in reversed(range(x.shape[0] - 1)):\n",
    "            discount_cumsum[t] = x[t] + gamma * discount_cumsum[t + 1]\n",
    "        return discount_cumsum\n",
    "\n",
    "    def __call__(self, features):\n",
    "        batch_size = len(features)\n",
    "        # this is a bit of a hack to be able to sample of a non-uniform distribution\n",
    "        batch_inds = np.random.choice(\n",
    "            np.arange(self.n_traj),\n",
    "            size=batch_size,\n",
    "            replace=True,\n",
    "            p=self.p_sample,  # reweights so we sample according to timesteps\n",
    "        )\n",
    "        # a batch of dataset features\n",
    "        s, a, r, d, rtg, timesteps, mask = [], [], [], [], [], [], []\n",
    "        \n",
    "        for ind in batch_inds:\n",
    "            # for feature in features:\n",
    "            feature = self.dataset[int(ind)]\n",
    "            si = random.randint(0, len(feature[\"rewards\"]) - 1)\n",
    "\n",
    "            # get sequences from dataset\n",
    "            s.append(np.array(feature[\"observations\"][si : si + self.max_len]).reshape(1, -1, self.state_dim))\n",
    "            a.append(np.array(feature[\"actions\"][si : si + self.max_len]).reshape(1, -1, self.act_dim))\n",
    "            r.append(np.array(feature[\"rewards\"][si : si + self.max_len]).reshape(1, -1, 1))\n",
    "\n",
    "            d.append(np.array(feature[\"dones\"][si : si + self.max_len]).reshape(1, -1))\n",
    "            timesteps.append(np.arange(si, si + s[-1].shape[1]).reshape(1, -1))\n",
    "            timesteps[-1][timesteps[-1] >= self.max_ep_len] = self.max_ep_len - 1  # padding cutoff\n",
    "            rtg.append(\n",
    "                self._discount_cumsum(np.array(feature[\"rewards\"][si:]), gamma=1.0)[\n",
    "                    : s[-1].shape[1]   # TODO check the +1 removed here\n",
    "                ].reshape(1, -1, 1)\n",
    "            )\n",
    "            if rtg[-1].shape[1] < s[-1].shape[1]:\n",
    "                print(\"if true\")\n",
    "                rtg[-1] = np.concatenate([rtg[-1], np.zeros((1, 1, 1))], axis=1)\n",
    "\n",
    "            # padding and state + reward normalization\n",
    "            tlen = s[-1].shape[1]\n",
    "            s[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, self.state_dim)), s[-1]], axis=1)\n",
    "            s[-1] = (s[-1] - self.state_mean) / self.state_std\n",
    "            a[-1] = np.concatenate(\n",
    "                [np.ones((1, self.max_len - tlen, self.act_dim)) * -10.0, a[-1]],\n",
    "                axis=1,\n",
    "            )\n",
    "            r[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), r[-1]], axis=1)\n",
    "            d[-1] = np.concatenate([np.ones((1, self.max_len - tlen)) * 2, d[-1]], axis=1)\n",
    "            rtg[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), rtg[-1]], axis=1) / self.scale\n",
    "            timesteps[-1] = np.concatenate([np.zeros((1, self.max_len - tlen)), timesteps[-1]], axis=1)\n",
    "            mask.append(np.concatenate([np.zeros((1, self.max_len - tlen)), np.ones((1, tlen))], axis=1))\n",
    "\n",
    "        s = torch.from_numpy(np.concatenate(s, axis=0)).float()\n",
    "        a = torch.from_numpy(np.concatenate(a, axis=0)).float()\n",
    "        r = torch.from_numpy(np.concatenate(r, axis=0)).float()\n",
    "        d = torch.from_numpy(np.concatenate(d, axis=0))\n",
    "        rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).float()\n",
    "        timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).long()\n",
    "        mask = torch.from_numpy(np.concatenate(mask, axis=0)).float()\n",
    "\n",
    "        return {\n",
    "            \"states\": s,\n",
    "            \"actions\": a,\n",
    "            \"rewards\": r,\n",
    "            \"returns_to_go\": rtg,\n",
    "            \"timesteps\": timesteps,\n",
    "            \"attention_mask\": mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmTRGPKYUVFG"
   },
   "source": [
    "### Step 5: Extending the Decision Transformer Model to include a loss function\n",
    "\n",
    "In order to train the model with the ðŸ¤— trainer class, we first need to ensure the dictionary it returns contains a loss, in this case L-2 norm of the models action predictions and the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "zIJCY3b3pQAh",
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1257368"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collator = DecisionTransformerGymDataCollator(dataset[\"train\"])\n",
    "\n",
    "config = DecisionTransformerConfig(state_dim=collator.state_dim, act_dim=collator.act_dim)\n",
    "model = TrainableDT(config)\n",
    "\n",
    "def num_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "num_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJJ2mr_cU4eE"
   },
   "source": [
    "### Step 6: Defining the training hyperparameters and training the model\n",
    "Here, we define the training hyperparameters and our Trainer class that we'll use to train our Decision Transformer model.\n",
    "\n",
    "This step takes about an hour, so you may leave it running. Note the authors train for at least 3 hours, so the results presented here are not as performant as the models hosted on the ðŸ¤— hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "nNzzKWuuU9I4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100000' max='100000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100000/100000 4:43:28, Epoch 6250/6250]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.042700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.031900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.030200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.028200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.027900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.027600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.027200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.027100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.026900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>0.026800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.026700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>0.026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.026500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95000</td>\n",
       "      <td>0.026400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.026200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:11<00:00,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1.0 eval: {'target_12.0_return_mean': -24.388857590408495, 'target_12.0_return_std': 0.3785924929925912, 'target_12.0_length_mean': -24.388857590408495, 'target_12.0_length_std': 0.3785924929925912}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:11<00:00,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 501.0 eval: {'target_12.0_return_mean': 10184.166463494808, 'target_12.0_return_std': 1499.0420624377882, 'target_12.0_length_mean': 10184.166463494808, 'target_12.0_length_std': 1499.0420624377882}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:11<00:00,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1001.0 eval: {'target_12.0_return_mean': 11110.21172932327, 'target_12.0_return_std': 186.07299326008567, 'target_12.0_length_mean': 11110.21172932327, 'target_12.0_length_std': 186.07299326008567}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:11<00:00,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1501.0 eval: {'target_12.0_return_mean': 11132.929309485986, 'target_12.0_return_std': 111.95761758702164, 'target_12.0_length_mean': 11132.929309485986, 'target_12.0_length_std': 111.95761758702164}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:11<00:00,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2001.0 eval: {'target_12.0_return_mean': 11063.84870537514, 'target_12.0_return_std': 226.18844467243147, 'target_12.0_length_mean': 11063.84870537514, 'target_12.0_length_std': 226.18844467243147}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:11<00:00,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2501.0 eval: {'target_12.0_return_mean': 10955.150628678279, 'target_12.0_return_std': 79.64473145686183, 'target_12.0_length_mean': 10955.150628678279, 'target_12.0_length_std': 79.64473145686183}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:11<00:00,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3001.0 eval: {'target_12.0_return_mean': 11088.433584288121, 'target_12.0_return_std': 117.3586700046007, 'target_12.0_length_mean': 11088.433584288121, 'target_12.0_length_std': 117.3586700046007}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:11<00:00,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3501.0 eval: {'target_12.0_return_mean': 10933.639805734687, 'target_12.0_return_std': 136.21261398159734, 'target_12.0_length_mean': 10933.639805734687, 'target_12.0_length_std': 136.21261398159734}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:11<00:00,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4001.0 eval: {'target_12.0_return_mean': 11271.795629340548, 'target_12.0_return_std': 219.80849500849882, 'target_12.0_length_mean': 11271.795629340548, 'target_12.0_length_std': 219.80849500849882}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:11<00:00,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4501.0 eval: {'target_12.0_return_mean': 11211.743321033076, 'target_12.0_return_std': 108.22947529685659, 'target_12.0_length_mean': 11211.743321033076, 'target_12.0_length_std': 108.22947529685659}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:11<00:00,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5001.0 eval: {'target_12.0_return_mean': 11363.736243529724, 'target_12.0_return_std': 92.20732713645, 'target_12.0_length_mean': 11363.736243529724, 'target_12.0_length_std': 92.20732713645}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:11<00:00,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5501.0 eval: {'target_12.0_return_mean': 11090.862641595084, 'target_12.0_return_std': 146.00222129211522, 'target_12.0_length_mean': 11090.862641595084, 'target_12.0_length_std': 146.00222129211522}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:11<00:00,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6001.0 eval: {'target_12.0_return_mean': 11195.28904575367, 'target_12.0_return_std': 60.4180799733843, 'target_12.0_length_mean': 11195.28904575367, 'target_12.0_length_std': 60.4180799733843}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:11<00:00,  2.24s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'target_12.0_return_mean': 11103.532312140353,\n",
       " 'target_12.0_return_std': 44.267410736544704,\n",
       " 'target_12.0_length_mean': 11103.532312140353,\n",
       " 'target_12.0_length_std': 44.267410736544704}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these params more or less match the ones used by the original DT paper\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output/\",\n",
    "    remove_unused_columns=False,\n",
    "    #num_train_epochs=120,\n",
    "    max_steps=100_000,\n",
    "    logging_strategy='steps',\n",
    "    save_strategy='no',\n",
    "    logging_steps=5000,\n",
    "    warmup_steps=0,\n",
    "    per_device_train_batch_size=64,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    #warmup_ratio=0.1,\n",
    "    optim=\"adamw_torch\",\n",
    "    dataloader_num_workers=12,\n",
    "    max_grad_norm=0.25,\n",
    "    tf32=True,\n",
    "    bf16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    data_collator=collator,\n",
    "    callbacks=[EvaluateCallback()],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "evaluate_episodes(5, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Mamba Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "y0NhIn4up26c",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import mujoco\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "TFPuiNy-qWnP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04489212  0.03232612  0.06034821 -0.17081618 -0.19477023 -0.05751681\n",
      "  0.0970142   0.03239178 11.0473385  -0.07997213 -0.32363245  0.3629689\n",
      "  0.42323524  0.40836537  1.1085011  -0.48743752 -0.07375081]\n"
     ]
    }
   ],
   "source": [
    "# build the environment\n",
    "directory = './video'\n",
    "device = \"cuda\"\n",
    "\n",
    "model = model.to(device)\n",
    "env = gym.make(\"HalfCheetah-v4\", render_mode='rgb_array')\n",
    "\n",
    "#env = gym.wrappers.RecordVideo(env, directory)\n",
    "max_ep_len = 1000\n",
    "scale = 1000.0  # normalization for rewards/returns\n",
    "TARGET_RETURN = 12000 / scale  # evaluation is conditioned on a return of 12000, scaled accordingly\n",
    "\n",
    "state_mean = collator.state_mean.astype(np.float32)\n",
    "state_std = collator.state_std.astype(np.float32)\n",
    "print(state_mean)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "# Create the decision transformer model\n",
    "\n",
    "# state_mean = torch.from_numpy(state_mean).to(device=device)\n",
    "# state_std = torch.from_numpy(state_std).to(device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_episodes(num_eval_episodes, model):\n",
    "    returns, lengths = [], []\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in tqdm(range(num_eval_episodes)):\n",
    "            ret, length = evaluate_episode_rtg(\n",
    "                env=env,\n",
    "                state_dim=state_dim,\n",
    "                act_dim=act_dim,\n",
    "                model=model,\n",
    "                scale=scale,\n",
    "                state_mean=state_mean,\n",
    "                state_std=state_std,\n",
    "                device=device,\n",
    "                target_return=TARGET_RETURN,\n",
    "            )\n",
    "\n",
    "            returns.append(ret)\n",
    "            lengths.append(ret)\n",
    "\n",
    "    return {\n",
    "        f'target_{TARGET_RETURN}_return_mean': np.mean(returns),\n",
    "        f'target_{TARGET_RETURN}_return_std': np.std(returns),\n",
    "        f'target_{TARGET_RETURN}_length_mean': np.mean(lengths),\n",
    "        f'target_{TARGET_RETURN}_length_std': np.std(lengths),\n",
    "    }\n",
    "\n",
    "class EvaluateCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if (int(state.epoch) - 1) % 500 == 0:\n",
    "            print('Epoch', state.epoch, 'eval:', evaluate_episodes(5, model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1230872\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100000' max='100000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100000/100000 4:48:11, Epoch 6250/6250]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.036200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.026100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.025300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.024700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.024200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.023800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.023400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.022700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.022200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.021900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.021700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>0.021500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.021300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>0.021200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.021100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95000</td>\n",
       "      <td>0.020900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.020800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:12<00:00,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1.0 eval: {'target_12.0_return_mean': -14.06919297013171, 'target_12.0_return_std': 1.1605751406392475, 'target_12.0_length_mean': -14.06919297013171, 'target_12.0_length_std': 1.1605751406392475}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:12<00:00,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 501.0 eval: {'target_12.0_return_mean': 11004.728973586252, 'target_12.0_return_std': 71.91044712426662, 'target_12.0_length_mean': 11004.728973586252, 'target_12.0_length_std': 71.91044712426662}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:13<00:00,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1001.0 eval: {'target_12.0_return_mean': 11008.887768843582, 'target_12.0_return_std': 83.51621808539541, 'target_12.0_length_mean': 11008.887768843582, 'target_12.0_length_std': 83.51621808539541}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:13<00:00,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1501.0 eval: {'target_12.0_return_mean': 11239.333516797022, 'target_12.0_return_std': 100.52138315673544, 'target_12.0_length_mean': 11239.333516797022, 'target_12.0_length_std': 100.52138315673544}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:13<00:00,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2001.0 eval: {'target_12.0_return_mean': 11195.615892787908, 'target_12.0_return_std': 71.89832828904474, 'target_12.0_length_mean': 11195.615892787908, 'target_12.0_length_std': 71.89832828904474}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:13<00:00,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2501.0 eval: {'target_12.0_return_mean': 11145.891078088494, 'target_12.0_return_std': 145.42430855113577, 'target_12.0_length_mean': 11145.891078088494, 'target_12.0_length_std': 145.42430855113577}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:13<00:00,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3001.0 eval: {'target_12.0_return_mean': 11120.522921012129, 'target_12.0_return_std': 57.643228258727056, 'target_12.0_length_mean': 11120.522921012129, 'target_12.0_length_std': 57.643228258727056}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:13<00:00,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3501.0 eval: {'target_12.0_return_mean': 11137.036670080712, 'target_12.0_return_std': 132.2095648109389, 'target_12.0_length_mean': 11137.036670080712, 'target_12.0_length_std': 132.2095648109389}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:13<00:00,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4001.0 eval: {'target_12.0_return_mean': 11135.456405894121, 'target_12.0_return_std': 117.30773801670622, 'target_12.0_length_mean': 11135.456405894121, 'target_12.0_length_std': 117.30773801670622}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:13<00:00,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4501.0 eval: {'target_12.0_return_mean': 11177.834986975333, 'target_12.0_return_std': 126.7074676400272, 'target_12.0_length_mean': 11177.834986975333, 'target_12.0_length_std': 126.7074676400272}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:13<00:00,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5001.0 eval: {'target_12.0_return_mean': 11234.28329622281, 'target_12.0_return_std': 123.76612037424367, 'target_12.0_length_mean': 11234.28329622281, 'target_12.0_length_std': 123.76612037424367}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:13<00:00,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5501.0 eval: {'target_12.0_return_mean': 11178.267474027167, 'target_12.0_return_std': 129.48385591572944, 'target_12.0_length_mean': 11178.267474027167, 'target_12.0_length_std': 129.48385591572944}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:13<00:00,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6001.0 eval: {'target_12.0_return_mean': 11177.703071772648, 'target_12.0_return_std': 25.172768386687757, 'target_12.0_length_mean': 11177.703071772648, 'target_12.0_length_std': 25.172768386687757}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100000, training_loss=0.02360807273864746, metrics={'train_runtime': 17297.5847, 'train_samples_per_second': 369.994, 'train_steps_per_second': 5.781, 'total_flos': 9008946000000000.0, 'train_loss': 0.02360807273864746, 'epoch': 6250.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = DecisionTransformerConfig(state_dim=collator.state_dim, act_dim=collator.act_dim)\n",
    "model = TrainableDM(config)\n",
    "\n",
    "print(num_params(model))\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    data_collator=collator,\n",
    "    callbacks=[EvaluateCallback()],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "evaluate_episodes(5, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNaj6bOkp3bt"
   },
   "source": [
    "### Step 7: Visualize the performance of the agent\n",
    "\n",
    "With mujoco_py, it'll take a little while to compile the first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:13<00:00,  2.61s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'target_12.0_return_mean': 11049.180788166927,\n",
       " 'target_12.0_return_std': 174.65680113676865,\n",
       " 'target_12.0_length_mean': 11049.180788166927,\n",
       " 'target_12.0_length_std': 174.65680113676865}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_episodes(5, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
